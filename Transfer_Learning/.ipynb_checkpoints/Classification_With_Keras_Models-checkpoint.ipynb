{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cf3fd4ff-4e14-44c1-985a-a128d74e7346",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MONSTER\\anaconda3\\envs\\image\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\MONSTER\\anaconda3\\envs\\image\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\MONSTER\\anaconda3\\envs\\image\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import collections\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import *\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "40fe005f-18ae-44e4-b2f7-4ef54e981c8e",
    "outputId": "b2affc46-47a1-42d8-c92c-6c663e586160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "# initialize the initial learning rate, number of epochs to train for,\n",
    "# and batch size\n",
    "INIT_LR = 1e-4\n",
    "EPOCHS = 20\n",
    "BS = 1\n",
    "# Composition EchogenicFoci Echogenity Margin Shape\n",
    "DIRECTORY = r\"D:/Datasets/BGYF_Dataset\"\n",
    "CATEGORIES = os.listdir(DIRECTORY)\n",
    "CATEGORY_SIZE = len(CATEGORIES)\n",
    "# grab the list of images in our dataset directory, then initialize\n",
    "# the list of data (i.e., images) and class images\n",
    "print(\"[INFO] loading images...\")\n",
    "\n",
    "data = []\n",
    "labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "693056da-3543-40e9-8fe1-79680c607810"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY, category)\n",
    "    filelist = os.listdir(path)\n",
    "    for fichier in filelist[:]: # filelist[:] makes a copy of filelist.\n",
    "        if not(fichier.endswith(\".jpg\")):\n",
    "            filelist.remove(fichier)\n",
    "    \n",
    "    for img in filelist:\n",
    "        img_path = os.path.join(path, img)\n",
    "        image = load_img(img_path, target_size=(224, 224))\n",
    "        \n",
    "        image = img_to_array(image)\n",
    "        image = preprocess_input(image)\n",
    "\n",
    "        data.append(image)\n",
    "        labels.append(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'E',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H',\n",
       " 'H']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "63c228d1-8e03-4067-8f26-7fdf673c49cd",
    "outputId": "f310cb0d-ab76-480b-887f-985ac5f46c82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'E': 90, 'H': 110})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "labels\n",
    "counter=collections.Counter(labels)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "194056d8-e5e7-4be2-960a-16c86c7581ec",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E'\n",
      " 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E'\n",
      " 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E'\n",
      " 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E'\n",
      " 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E'\n",
      " 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# perform one-hot encoding on the labels\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define example\n",
    "values = array(labels)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "labels = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(labels)\n",
    "\n",
    "\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)\n",
    "\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "    test_size=0.30, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d8ad3ae9-795b-4432-b802-68b157825b8c"
   },
   "outputs": [],
   "source": [
    "# construct the training image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "678d09da-7c5b-40b4-943e-63f2e1b60580",
    "outputId": "58ea2e47-09f6-4f39-f27b-74b227d30271",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training head...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MONSTER\\anaconda3\\envs\\image\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "names = [\"Xception\",\"VGG16\",\"VGG19\",\"ResNet50\",\"ResNet101\",\"ResNet152\",\"ResNet50V2\",\"ResNet101V2\",\"ResNet152V2\",\"InceptionV3\",\"InceptionResNetV2\",\"MobileNet\",\n",
    "         \"MobileNetV2\",\"DenseNet121\",\"DenseNet169\",\"DenseNet201\",\"NASNetMobile\",\"NASNetLarge\",\"EfficientNetB0\",\"EfficientNetB1\",\"EfficientNetB2\",\"EfficientNetB3\",\n",
    "         \"EfficientNetB4\",\"EfficientNetB5\",\"EfficientNetB6\",\"EfficientNetB7\"]\n",
    "for i in range(26):\n",
    "    NAME = \"i{}-{}-{}\".format(i,names[i],int(time.time()))\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "    if i == 0:\n",
    "        baseModel = Xception(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 1:\n",
    "        baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 2:\n",
    "        baseModel = VGG19(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 3:\n",
    "        baseModel = ResNet50(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 4:\n",
    "        baseModel = ResNet101(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 5:\n",
    "        baseModel = ResNet152(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 6:\n",
    "        baseModel = ResNet50V2(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 7:\n",
    "        baseModel = ResNet101V2(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 8:\n",
    "        baseModel = ResNet152V2(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 9:\n",
    "        baseModel = InceptionV3(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 10:\n",
    "        baseModel = InceptionResNetV2(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 11:\n",
    "        baseModel = MobileNet(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 12:\n",
    "        baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 13:\n",
    "        baseModel = DenseNet121(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 14:\n",
    "        baseModel = DenseNet169(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 15:\n",
    "        baseModel = DenseNet201(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 16:\n",
    "        baseModel = NASNetMobile(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 17:\n",
    "        baseModel = NASNetLarge(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 18:\n",
    "        baseModel = EfficientNetB0(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 19:\n",
    "        baseModel = EfficientNetB1(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 20:\n",
    "        baseModel = EfficientNetB2(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 21:\n",
    "        baseModel = EfficientNetB3(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 22:\n",
    "        baseModel = EfficientNetB4(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 23:\n",
    "        baseModel = EfficientNetB5(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 24:\n",
    "        baseModel = EfficientNetB6(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "    elif i == 25:\n",
    "        baseModel = EfficientNetB7(weights=\"imagenet\", include_top=False,\n",
    "            input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "    # construct the head of the model that will be placed on top of the\n",
    "    # the base model\n",
    "    headModel = baseModel.output\n",
    "    try:\n",
    "        headModel = AveragePooling2D(pool_size=(7, 7),data_format=\"channels_first\")(headModel)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    headModel = Flatten(name=\"flatten\")(headModel)\n",
    "    headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "    headModel = Dropout(0.5)(headModel)\n",
    "    headModel = Dense(CATEGORY_SIZE, activation=\"sigmoid\")(headModel)\n",
    "       \n",
    "    headModel = baseModel.output\n",
    "    headModel = Flatten(name=\"flatten\")(headModel)\n",
    "    headModel = Dense(CATEGORY_SIZE, activation=\"sigmoid\")(headModel)\n",
    "\n",
    "    # place the head FC model on top of the base model (this will become\n",
    "    # the actual model we will train)\n",
    "    model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "    # loop over all layers in the base model and freeze them so they will\n",
    "    # *not* be updated during the first training process\n",
    "    for layer in baseModel.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # compile our model\n",
    "    print(\"[INFO] compiling model...\")\n",
    "    opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "        metrics=[\"accuracy\"])\n",
    "    \n",
    "    # train the head of the network\n",
    "    print(\"[INFO] training head...\")\n",
    "    model.fit(\n",
    "        aug.flow(trainX, trainY, batch_size=BS),\n",
    "        steps_per_epoch=len(trainX) // BS,\n",
    "        validation_data=(testX, testY),\n",
    "        validation_steps=len(testX) // BS,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[tensorboard])\n",
    "    \n",
    "    # make predictions on the testing setk\n",
    "    print(\"[INFO] evaluating network...\")\n",
    "    predIdxs = model.predict(testX, batch_size=BS)\n",
    "\n",
    "    # for each image in the testing set we need to find the index of the\n",
    "    # label with corresponding largest predicted probability\n",
    "    predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "    # show a nicely formatted classification report\n",
    "#    print(classification_report(testY.argmax(axis=1), predIdxs,\n",
    "#        target_names=lb.classes_))\n",
    "\n",
    "    # serialize the model to disk\n",
    "#    print(\"[INFO] saving multiclass model...\")\n",
    "#    try:\n",
    "#        model.save(\"aflotoksin.h5\", save_format=\"h5\")\n",
    "#    except:\n",
    "#        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype('float32').reshape((-1,1))\n",
    "y_test = np.asarray(test_labels).astype('float32').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "image"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
